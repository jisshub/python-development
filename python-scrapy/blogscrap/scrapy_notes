install scrapy
-----------
pip install scrapy

Creating a project
---------------------
scrapy startproject tutorial

tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/

        __init__.py      # project's Python module, you'll import your code from here

        items.py          # project items definition file

        middlewares.py    # project middlewares file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py

Our first Spider
------------------
Spiders are classes that you define and that Scrapy uses to scrape information from a website

A Sample scrapt progaram
0-------------------------


class Sample(scrapy.Spider):
    name = "posts"
        # urls to scrap from
    start_urls = ["https://blog.scrapinghub.com/page/1/",
                  "https://blog.scrapinghub.com/page/2/", ]

     # use parse() to process the response we get
    # and return the scraped data
    def parse(self, response):
        # get page of each url in the start_urls list,
        page_no = response.url.split("/")[-1]
        # a filename for pages to scrap
        filename = f"page-{page_no}.html"
        # write to these files
        with open(filename, mode="wb") as f:
            # write the body of the page to files
            f.write(response.body)

runnig the project
------------------
scrapy crawl <spider_name>



working dwith srcapy shell
-------------------------
scrapy shell <url_to_crawl>

command commands
----------
1. using css selectors:

>> response.css("title")
    -- returns selector list that wraps around
        html element 'title'



selecting by class name
-----------------------------
response.css(".post-header").get()
    -- first post-header class element
response.css(".post-header").getall()
    -- all post-header class element
response.css(".post-header a").get()
    -- first a tag in post header class
response.css(".post-header a::text").get()
    -- get first text of a tag.

response.css(".post-header a::text").getall()
    -- get all text of a tag.


using regex
----------
response.css('p::text').re('scraping')
    -- gets a list with value as scraping

response.css('p::text').re('[a-z]+')
    -- gets a list of lowercase words and letters.


using xpath selector
-------------------
xpath:
    It is a syntax or language for finding any element
    on the web page using XML path expression

response.xpath('//h3)
-- gets h3 tag in a list

response.xpath('//h3/text()').extract()
   -- list of text values in h3 tags

response.xpath('xpath in html').extract
    -- list of corresponding tags

response.xpath('//span[@class="author"]').extract()
 - gets all span element with class as "author"