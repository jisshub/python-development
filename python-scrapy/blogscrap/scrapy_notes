install scrapy
-----------
pip install scrapy

Creating a project
---------------------
scrapy startproject tutorial

tutorial/
    scrapy.cfg            # deploy configuration file

    tutorial/

        __init__.py      # project's Python module, you'll import your code from here

        items.py          # project items definition file

        middlewares.py    # project middlewares file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py

Our first Spider
------------------
Spiders are classes that you define and that Scrapy uses to scrape information from a website

A Sample scrapt progaram
0-------------------------


class Sample(scrapy.Spider):
    name = "posts"
        # urls to scrap from
    start_urls = ["https://blog.scrapinghub.com/page/1/",
                  "https://blog.scrapinghub.com/page/2/", ]

     # use parse() to process the response we get
    # and return the scraped data
    def parse(self, response):
        # get page of each url in the start_urls list,
        page_no = response.url.split("/")[-1]
        # a filename for pages to scrap
        filename = f"page-{page_no}.html"
        # write to these files
        with open(filename, mode="wb") as f:
            # write the body of the page to files
            f.write(response.body)

runnig the project
------------------
cd to the directrory where scrapy.cfg file exists,
later run the below command,

scrapy crawl <spider_name>



working dwith srcapy shell
-------------------------
scrapy shell <url_to_crawl>

command commands
----------
1. using css selectors:

>> response.css("title")
    -- returns selector list that wraps around
        html element 'title'



selecting by class name
-----------------------------
response.css(".post-header").get()
    -- first post-header class element
response.css(".post-header").getall()
    -- all post-header class element
response.css(".post-header a").get()
    -- first a tag in post header class
response.css(".post-header a::text").get()
    -- get first text of a tag.

response.css(".post-header a::text").getall()
    -- get all text of a tag.


using regex
----------
response.css('p::text').re('scraping')
    -- gets a list with value as scraping

response.css('p::text').re('[a-z]+')
    -- gets a list of lowercase words and letters.


using xpath selector
-------------------
xpath:
    It is a syntax or language for finding any element
    on the web page using XML path expression

response.xpath('//h3)
-- gets h3 tag in a list

response.xpath('//h3/text()').extract()
   -- list of text values in h3 tags

response.xpath('xpath in html').extract
    -- list of corresponding tags

response.xpath('//span[@class="author"]').extract()
 - gets all span element with class as "author"


 sample program
 -------------

 post_item = response.css('.post-item')

 >>> for each_post in post_item:
...     title = each_post.css('.post-header a::text').get()
...     date = each_post.css('.post-header a::text')[1].get()
...     author = each_post.css('.post-header a::text')[2].get()
        print(dict(title = title, date=date, author=author))
...


to store the scarped to json

-> scrap craw <spidername> -o filename.json


Scraping multiple pages - full code
-------------------------
from scrapy import Spider, Request

import json


class PostSpider(Spider):
    name = "posts"
    start_urls = ['https://blog.scrapinghub.com/']

    def parse(self, response):
        # get selector list post items
        post_item = response.css(".post-item")
        # iterate thru each post_item
        for each_post_data in post_item:
            # each_post_date is a selector type , thus we can apply css selector
            # here v will yield the result in a dictionary
            yield {
                # get title of each post
                "title": each_post_data.css('.post-header a::text').get(),
                # get date on each post
                "date": each_post_data.css('.post-header a::text')[1].get(),
                # get author of each post
                "author": each_post_data.css('.post-header a::text')[2].get()
            }
            # get url of next page
        next_page = response.css("a.next-posts-link::attr(href)").get()
        if next_page:
            # if url exists join url with response object
            next_page = response.urljoin(next_page)
            # call Request() which accepts a url and a callback function parse().
            # later, yield the result
            yield Request(next_page, callback=self.parse)
            # thus this url will processed and crawled by parse() again.


